# HITL-and-Human-Feedback
Here are the differences i discovered while trying to wrap my head around (Human in the loop) and (Human feedback).

Question Answered: Is human in the loop and human feedback the same thing?

Human-in-the-loop (HITL) and Human feedback are not the same thing, although they are deeply related and often used together in AI development.

* **Human-in-the-Loop (HITL)** is a broader **framework or methodolgy** where humans are part of the AI process
- training
- monitoring
- or acting on results

* **Human Feedback** is the specific
- input
- evaluation
- or signal
that the human provides **while in that loop**.

Think of it this way: **HITL is the structure, and human feedback in the content** that flows through that structure. 

# Human-in-the-Loop (HITL) - The "Where" and "When"

HITL embeds human expertise at key points in the AI lifesysle, such as data labeling, model training, or decision-making, ensuring that humans can review or correct AI outcomes.

* **Key Aspect:** It defines that workflow.
* **Common Applications:** Data annotation, active learning (asking humans for input on difficult cases), and human approval for AI-driven decisions.
* **Goal:** To improve model accuracy and ensure safety

# Human Feedback - The "What"

Human feedback is the specific data generated by humans in response to an AI's behavior or output. This feedback is used to update the model to be more accurate, helpful, or safe.

* Key Aspect: It is the data or signal
* Forms of Feedback: Thumbsup/down, ranking responses (1st,2nd,3rd), correcting an answer, or labeling an image.
* Goal: To train, refine, or align the model.

# Key Differences Summary

| Feature | Human-in-the-loop | Human Feedback |
|---|---|---|
| Definition | A design pattern/structure for AI | The information/data given by humans |
| Scope | The overall process (design, train, run) | Specific evaluations of model output |
| Function | Integrates humans to manage, review, or control | Guides the AI toward better behavior |
| Exampple | An AI flags a suspicious transation for a human to review | The human says "This is not fraud" (feedback) |

# The Overlap: Reinforcement Learning from Human Feedback (RLHF)

While distinct, they work together in methods like RLHF. In RLHF, the "loop" is that the AI generates multiple outputs, the human provides "feedback" (by ranking them), and the model learns to improve from that data.

Example: [Waymo Writeup](https://github.com/ykolie/HITL-and-Human-Feedback/blob/main/example-waymo.md)
